# -*- coding: utf-8 -*-
"""fake_news classification with large dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1spfKtTNCNJEv9oJKz31i7fGD8xDzfj-G
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud, STOPWORDS
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import plotly.express as px
import string
import warnings
warnings.filterwarnings('ignore')

"""if data file upload is hamperd any how, it will create problem and show !!ERROR!!"""

df=pd.read_csv('/content/WELFake_Dataset.csv')
df

df['text'][0]

df['text'][72131]

df.isnull().sum()

df.info()

"""DROPPING UNNAMED : 0 COLUMN"""

df.drop('Unnamed: 0',axis=1,inplace=True)
df

to modify the null value we need to use fillna() function. In function body, we need to upload null string

#uploading null value in empty text column
df.fillna(' ',inplace=True)
features = df[['title','text']]
labels= df['label']

"""the data frame which is stored in "df variable" spilit into 2 variable. i)feature, ii)labels.
feature = this variable contains 'title' ,
         'text' column
label = this variable contains 'label'   
         column         
"""

df.isnull().sum()

"""Now we draw a BAR to see the lable column's result. Is the dataset balanced or not?"""

sns.countplot(x='label', data=df)
 plt.show()

"""Now we use direct function to see the lable column's result. Is the dataset balanced or not?"""

df['label'].value_counts()
# OR,,, a =df['label'].value_counts()
      # print(a)

"""Now we draw a pie chart to see the lable column's result. Is the dataset balanced or not?
label 0 = SPAM
label 1 = HAM


"""

a=df['label'].value_counts()
transaction = a.index
quantity=a.values
fig = px.pie(df, names=transaction, values = quantity, hole=0.5, title="spam & ham status")
fig.show()

"""by Word cloud we can handle which words mostly used in the dataset"""

def plot_wordcloud(text, title):
  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
  plt.figure(figsize=(10,5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.title(title)

fake =' '.join(df[df['label'] == 0]['text'])
real =' '.join(df[df['label'] == 1]['text'])

plot_wordcloud(fake, 'fake news')
plt.show()

plot_wordcloud(real,'real news')
plt.show()

"""TEXT  PREPROCESSING

"""

text_column='text'
label_column='label'

"""generally stop words never have potential value."""

import nltk
nltk.download('stopwords')

stopword = set(stopwords.words('english'))
stopword

def preprocess_text(text):
         #REMOVE Punctuation (remove_punc ei variable e string.punctuation libraryr kono punctuation thakbe na)
  remove_punc = [char for char in text if char not in string.punctuation]
  clean_words = ''.join(remove_punc)
         # REMOVE stopwords (clean_words variable er modde kono punctuation nai, ei variable ke amra split kore er theke stop word ke alada korlam)
  text= ([word for word in clean_words.split() if word.lower() not in stopword ])
         #jara jara stopwords na tara text variable e bosbe
  return text

df[text_column]= df[text_column].apply(preprocess_text)
df[text_column]

"""LEMMATIZATION
example= "hackers word" will be "hacker"
"""

import nltk
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
  lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text])
  return lemmatized_text

df[text_column]= df[text_column].apply(lemmatize_text)
df[text_column]

vectorizer = TfidfVectorizer()
x = vectorizer.fit_transform(df[text_column])
y= df[label_column]

"""TRAIN / TEST Split"""

xtrain, xtest, ytrain, ytest = train_test_split(x, labels, test_size=0.2, random_state=42)

def evaluate_model(model, xtest, ytest):
  y_pred = model.predict(xtest)
  accuracy= accuracy_score(ytest, y_pred)
  cm=confusion_matrix(ytest, y_pred)
  prob= model.predict_proba(xtest)[:,1]
  roc_auc= roc_auc_score(ytest, prob)
  fpr, tpr, rhresholds = roc_curve(ytest, prob)
  precision,recall, _ =precision_recall_curve(ytest,prob)
  pr_auc= auc(recall,precision)
  return {
      'Accuracy':accuracy,
      'Confusion Matrix':cm,

  }

"""MULTINOMIAL NAIVE BAYES"""

mnb=MultinomialNB(alpha=1.0,fit_prior=True,class_prior= None)
mnb.fit(xtrain,ytrain)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import auc
nb_result=evaluate_model(mnb,xtest,ytest)
nb_result

"""EVALUTE MANUALLY

"""

cm1= confusion_matrix(ytest, mnb.predict(xtest))
cm1

"""true positive = 6528 and true negative= 6164"""

b=classification_report(ytest, mnb.predict(xtest))
print(b)

LOGISTIC REGRESSION

lr=LogisticRegression()
lr.fit(xtrain,ytrain)

b=classification_report(ytest, lr.predict(xtest))
print(b)

cm= confusion_matrix(ytest, lr.predict(xtest))
cm

"""bam kona barle good... karon tokhon true positive=6674 and true negative=7071 is high

BOURNALLE
"""

B= BernoulliNB()
B.fit(xtrain,ytrain)

c=classification_report(ytest, B.predict(xtest))
print(c)

cm3= confusion_matrix(ytest, B.predict(xtest))
cm3

"""NOW WE GET SOME I/P FROM USER  """

#list e 3 ta model rakhlam
MODELS=[mnb, lr, B ]

random_text=input()
preprocessed_text=preprocess_text(random_text)
lemmatized_text=lemmatize_text(preprocessed_text)
text_vector=vectorizer.transform([lemmatized_text])

for model in MODELS:
  prediction= model.predict(text_vector)
  print(f"model:{type(model).__name__}")
  print("Prediction:",prediction)

"""Deploy the model in web.
how to save this file in pickle
"""

import pickle
#define a file path where u want to save the model
model_file_path= 'logistic_regression_model.pkl'
#save the model to the file

with open (model_file_path,'wb') as model_file:
     pickle.dump(lr,model_file )

"""LOAD THE MODEL"""

with open(model_file_path,'rb') as model_file:
  loaded_model=pickle.load(model_file)

loaded_model.predict(xtest)

!pip install scikit-learn
loaded_model.score(xtest,ytest)

loaded_model.predict(text_vector)